[TOC]
#  参考文献
零基础入门机器学习概率论
>1. [基本概念](https://m.jqr.com/news/009617)
>2. [最大似然估计](https://m.jqr.com/news/009637)
>3. [贝叶斯推断](https://m.jqr.com/news/009642)
>4. [边缘化](https://www.jqr.com/article/000065)
>5. [基本公理](https://www.jqr.com/article/000469)
>6. [概率分布](https://www.jqr.com/article/000474)

# 一些必要的概念

## 独立同分布

在随机过程中，任何时刻的取值都为随机变量，如果这些随机变量服从同一分布，并且互相独立，那么这些随机变量是独立同分布。如果随机变量X1和X2独立，是指X1的取值不影响X2的取值，X2的取值也不影响X1的取值且随机变量X1和X2服从同一分布，这意味着X1和X2具有相同的分布形状和相同的分布参数，对离散随机变量具有相同的分布律，对连续随机变量具有相同的概率密度函数，有着相同的分布函数，相同的期望、方差。

**如实验条件保持不变，一系列的抛硬币的正反面结果即是独立同分布。**多次重复试验的伯努利试验既是独立同分布。

## 先验概率（prior probability）和后验概率（posterior probability）

简单地说，生而知之就是先验（transcendental）。就像各种穿越剧中所讲主角从小就知道很多东西，但他并没有经历过（从旁人的角度看来）。这些知识是先于经验得到的，与此相对，正常的知识都是晚于经验得到的，称为后验。在此不考究经院学者们是怎么捣鼓这个概念的，康德的说法搞得我头都大了。

先验概率是指根据以往经验和分析得到的概率，如全概率公式，它往往作为"由因求果"问题中的"因"出现的概率。

后验概率是指在得到“结果”的信息后重新修正的概率，是“执果寻因”问题中的"果"。先验概率与后验概率有不可分割的联系，后验概率的计算要以先验概率为基础。

事情还没有发生，要求这件事情发生的可能性的大小，是先验概率。事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小，是后验概率。

后验概率的计算要以先验概率为基础。后验概率可以根据通过贝叶斯公式，用先验概率和似然函数计算出来。

## 概率学派和贝叶斯学派



## 范数

## 标准化(Standardization)和归一化(normalization)

1 概念
   归一化：１）把数据变成(０，１)或者（1,1）之间的小数。主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速。２）把有量纲表达式变成无量纲表达式，便于不同单位或量级的指标能够进行比较和加权。归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。
   标准化：在机器学习中，我们可能要处理不同种类的资料，例如，音讯和图片上的像素值，这些资料可能是高维度的，资料标准化后会使每个特征中的数值平均变为0(将每个特征的值都减掉原始资料中该特征的平均)、标准差变为1，这个方法被广泛的使用在许多机器学习算法中(例如：支持向量机、逻辑回归和类神经网络)。
   中心化：平均值为0，对标准差无要求
   归一化和标准化的区别：归一化是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响。它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。
   标准化和中心化的区别：标准化是原始分数减去平均数然后除以标准差，中心化是原始分数减去平均数。 所以一般流程为先中心化再标准化。
   无量纲：我的理解就是通过某种方法能去掉实际过程中的单位，从而简化计算。

2 为什么要归一化/标准化？
   如前文所说，归一化/标准化实质是一种线性变换，线性变换有很多良好的性质，这些性质决定了对数据改变后不会造成“失效”，反而能提高数据的表现，这些性质是归一化/标准化的前提。比如有一个很重要的性质：线性变换不会改变原始数据的数值排序。
 （1）某些模型求解需要
   1）在使用梯度下降的方法求解最优化问题时， 归一化/标准化后可以加快梯度下降的求解速度，即提升模型的收敛速度。如左图所示，未归一化/标准化时形成的等高线偏椭圆，迭代时很有可能走“之”字型路线（垂直长轴），从而导致迭代很多次才能收敛。而如右图对两个特征进行了归一化，对应的等高线就会变圆，在梯度下降进行求解时能较快的收敛。


## 正则化（regularization）

> [什么是正则化](https://charlesliuyx.github.io/2017/10/03/[直观详解]什么是正则化/)

经典的数学物理方程定解问题中，人们只研究适定问题。适定问题是指定解满足下面三个要求的问题：① 解是存在的；② 解是唯一的；③ 解连续依赖于定解条件，即解是稳定的。这三个要求中，只要有一个不满足，则称之为不适定问题（ill-posed problem）。

指在[线性代数](https://baike.baidu.com/item/线性代数/800)理论中，[不适定问题](https://baike.baidu.com/item/不适定问题)通常是由一组线性代数[方程](https://baike.baidu.com/item/方程/6306)定义的，而且这组方程组通常来源于有着很大的条件数的不适定反问题。大[条件数](https://baike.baidu.com/item/条件数/5293168)意味着舍入误差或其它误差会严重地影响问题的结果。

正则化就是对最小化经验误差函数上加约束，这样的约束可以解释为先验知识(正则化参数等价于对参数引入先验分布)。约束有引导作用，在优化误差函数的时候倾向于选择满足约束的梯度减少的方向，使最终的解倾向于符合先验知识(如一般的l-norm先验，表示原问题更可能是比较简单的，这样的优化倾向于产生参数值量级小的解，一般对应于稀疏参数的平滑解)。




## 训练数据集和测试数据集

![img](https://img-blog.csdn.net/20180417102757632)

一般做预测分析时，会将数据分为两大部分。一部分是训练数据，用于构建模型，一部分是测试数据，用于检验模型。但是，有时候模型的构建过程中也需要检验模型，辅助模型构建，所以会将训练数据在分为两个部分：1）训练数据；2）验证数据（Validation Data）。验证数据用于负责模型的构建。典型的例子是用K-Fold Cross Validation裁剪决策树，求出最优叶节点数，防止过渡拟合（Overfitting）。下面形式的描述一下前面提到的3类数据：

训练数据（Test Data）：用于模型构建
验证数据（Validation Data）：可选，用于辅助模型构建，可以重复使用。
测试数据（Test Data）：用于检测模型构建，此数据只在模型检验时使用，用于评估模型的准确率。绝对不允许用于模型构建过程，否则会导致过渡拟合

# 最大似然估计和贝叶斯推断

> [零基础概率论入门：最大似然估计](<https://m.jqr.com/news/009637>)
> [零基础概率论入门：贝叶斯推断](https://m.jqr.com/news/009642)
>[最大似然估计和最大后验估计](https://www.cnblogs.com/shixisheng/p/7136890.html)


在学习中，最大似然估计、最大后验估计和贝叶斯推断有很深的联系

# 正则与先验的等价证明



# 岭回归和lasso回归

> <https://blog.csdn.net/Joker_sir5/article/details/82756089>

# 梯度下降法

前向反向传播的推导

剑指offer LeetCode 中有关数据结构的  